{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install tqdm gensim keras nltk numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jW4jw3EDC8SR"
   },
   "source": [
    "## Sentiment Analysis on Twitter Data using Word2Vec (gensim) in Keras\n",
    "\n",
    "Sentiment Analysis refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine.[[Source: Wikipedia](https://en.wikipedia.org/wiki/Sentiment_analysis)]\n",
    "\n",
    "I attempt here to perform sentiment analysis using **Word2Vec** Text Embedding from [**gensim**](https://github.com/RaRe-Technologies/gensim).\n",
    "\n",
    "The analysis and training is performed on 400,000 Tweets which are either **Positive** or **Negative**\n",
    "\n",
    "With training on 400,000 Tweets, using word2vec, I was able to achieve an accuracy of approximately **69%**\n",
    "\n",
    "### Preprocessing Tweets\n",
    "\n",
    "Dataset is read from .txt file and then shuffled for mainting random distribution.\n",
    "\n",
    "Labels are then generated from each tweet.\n",
    "\n",
    "Finally all of the tweets are tokenized (`RegexpTokenizer()`) and then Lemmatized (`WordNetLemmatizer()`) for only storing the root words. \n",
    "\n",
    "All the variables or lists are deleted to save memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1488165,
     "status": "ok",
     "timestamp": 1532865160489,
     "user": {
      "displayName": "MANDAV",
      "photoUrl": "//lh4.googleusercontent.com/-Zi5_ERBmNoM/AAAAAAAAAAI/AAAAAAAAASc/1pLc_S0nstI/s50-c-k-no/photo.jpg",
      "userId": "113535474679329766874"
     },
     "user_tz": -330
    },
    "id": "LOgOLhHXU9EL",
    "outputId": "fb335d9f-3862-4d39-d307-8596320594a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 30/400000 [00:00<22:15, 299.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Labels ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [22:22<00:00, 297.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/400000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Lemmatizing ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [02:19<00:00, 2871.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "random.seed(1000)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer('[a-zA-Z0-9]\\w+')\n",
    "\n",
    "pos_tweets = []\n",
    "neg_tweets = []\n",
    "\n",
    "with open('pos_1.2M.txt', 'r', buffering=1000) as f:\n",
    "    pos_tweets = f.readlines()\n",
    "\n",
    "with open('neg_1.2M.txt', 'r', buffering=1000) as f:\n",
    "    neg_tweets = f.readlines()\n",
    "\n",
    "pos_tweets = pos_tweets[:200000]\n",
    "neg_tweets = neg_tweets[:200000]\n",
    "  \n",
    "print('Shuffling ..')\n",
    "tweets_unclean = list(pos_tweets) + list(neg_tweets)\n",
    "random.shuffle(tweets_unclean)\n",
    "\n",
    "print('Generating Labels ..')\n",
    "labels = []\n",
    "\n",
    "with tqdm(total=len(tweets_unclean)) as pbar:\n",
    "    for tweet in tweets_unclean:\n",
    "        if tweet in pos_tweets:\n",
    "              labels.append(1)\n",
    "        else:\n",
    "              labels.append(0)\n",
    "\n",
    "        pbar.update(1)\n",
    "    \n",
    "del pos_tweets\n",
    "del neg_tweets\n",
    "\n",
    "print('Tokenizing ..')\n",
    "tweets = [tokenizer.tokenize(tweet.lower()) for tweet in tweets_unclean]\n",
    "\n",
    "print('Done.')\n",
    "\n",
    "tweets = []\n",
    "\n",
    "print('Lemmatizing ..')\n",
    "\n",
    "with tqdm(total=len(tweets_unclean)) as pbar:\n",
    "    for tweet in tweets_unclean:\n",
    "        lemmatized = [lemmatizer.lemmatize(word) for word in tweet]\n",
    "        tweets.append(lemmatized)\n",
    "        pbar.update(1)\n",
    "\n",
    "del tweets_unclean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e7eC6qc7DNyu"
   },
   "source": [
    "### Generating Word2Vec and storing the Model\n",
    "\n",
    "Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space. [[Source: Wikipedia](https://en.wikipedia.org/wiki/Word2Vec)]\n",
    "\n",
    "Docs in Gensim: [models.word2vec](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "\n",
    "Word2Vec has 2 important models inside: Skip-Grams and Continous Bag-of-Words(CBOW)\n",
    "\n",
    "### Skip-Grams:\n",
    "In Skip-Gram model, we take a centre word and a window of context words  or neighbors within the context window and we try to predict context words for each centre word. The model generates a probability distribution i.e., probability of a word appearing in context given centre word and the task here is to choose the vector representation to maximize the probability.\n",
    "\n",
    "![Skip-Gram Model](skip-gram-model.png)\n",
    "\n",
    "\n",
    "![Example](skip-gram-example.png)\n",
    "\n",
    "\n",
    "### Continous Bag-of-Words (CBOW): \n",
    "CBOW is opposite of Skip-Grams. We attempt to predict the centre word from the given context i.e., we try to predict the centre word by summing vectors of surrounding words.\n",
    "\n",
    "![Continous Bag-of-Words](CBOW-model.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "2w0wWGDhdeHh"
   },
   "outputs": [],
   "source": [
    "vector_size = 256\n",
    "window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1122267,
     "status": "ok",
     "timestamp": 1532866283920,
     "user": {
      "displayName": "MANDAV",
      "photoUrl": "//lh4.googleusercontent.com/-Zi5_ERBmNoM/AAAAAAAAAAI/AAAAAAAAASc/1pLc_S0nstI/s50-c-k-no/photo.jpg",
      "userId": "113535474679329766874"
     },
     "user_tz": -330
    },
    "id": "waUN7DMLWW_K",
    "outputId": "00c1b1ee-b8b4-47b0-a47d-efbf35f3ad84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Word2Vec Vectors ..\n",
      "Word2Vec Created in 1120.455406665802 seconds.\n",
      "Word2Vec Model saved at word2vec.model\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import time\n",
    "\n",
    "word2vec_model = 'word2vec.model'\n",
    "\n",
    "print('Generating Word2Vec Vectors ..')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model = Word2Vec(sentences=tweets, size=vector_size, window=window, negative=20, iter=50, workers=4)\n",
    "\n",
    "print('Word2Vec Created in {} seconds.'.format(time.time() - start))\n",
    "\n",
    "model.save(word2vec_model)\n",
    "print('Word2Vec Model saved at {}'.format(word2vec_model))\n",
    "\n",
    "# Got to clear the memory!\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7aYHPnlKk7gy"
   },
   "outputs": [],
   "source": [
    "# Load the saved model!\n",
    "model = Word2Vec.load(word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mwXQ37rJsTzo"
   },
   "outputs": [],
   "source": [
    "x_vectors = model.wv\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 964,
     "status": "ok",
     "timestamp": 1532866423084,
     "user": {
      "displayName": "MANDAV",
      "photoUrl": "//lh4.googleusercontent.com/-Zi5_ERBmNoM/AAAAAAAAAAI/AAAAAAAAASc/1pLc_S0nstI/s50-c-k-no/photo.jpg",
      "userId": "113535474679329766874"
     },
     "user_tz": -330
    },
    "id": "hmy7ziZrsXGx",
    "outputId": "6eba8965-6e45-4940-8846-d2049eb6dca5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 400000)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels), len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9x7HSbVhEJh6"
   },
   "source": [
    "### Dataset Partition\n",
    "\n",
    "Spliting the tweets and labels in `(x_train, y_train)` and `(x_test, y_test)` with 90% for training and 10% for testing from all the tweets.\n",
    "\n",
    "Maximum number of tokens allowed for each tweet is set to be 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8305,
     "status": "ok",
     "timestamp": 1532866438403,
     "user": {
      "displayName": "MANDAV",
      "photoUrl": "//lh4.googleusercontent.com/-Zi5_ERBmNoM/AAAAAAAAAAI/AAAAAAAAASc/1pLc_S0nstI/s50-c-k-no/photo.jpg",
      "userId": "113535474679329766874"
     },
     "user_tz": -330
    },
    "id": "OPZ1DhxNueMQ",
    "outputId": "a1782ccc-28cc-4f0d-b719-c3ddd473eb71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "\n",
    "train_size = int(0.9*(len(tweets)))\n",
    "test_size = int(0.1*(len(tweets)))\n",
    "\n",
    "max_no_tokens = 15\n",
    "\n",
    "indexes = set(np.random.choice(len(tweets), train_size + test_size, replace=False))\n",
    "\n",
    "x_train = np.zeros((train_size, max_no_tokens, vector_size), dtype=K.floatx())\n",
    "y_train = np.zeros((train_size, 2), dtype=np.int32)\n",
    "\n",
    "x_test = np.zeros((test_size, max_no_tokens, vector_size), dtype=K.floatx())\n",
    "y_test = np.zeros((test_size, 2), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ADEodjHivhGe"
   },
   "outputs": [],
   "source": [
    "for i, index in enumerate(indexes):\n",
    "    for t, token in enumerate(tweets[index]):\n",
    "        if t >= max_no_tokens:\n",
    "            break\n",
    "      \n",
    "        if token not in x_vectors:\n",
    "            continue\n",
    "    \n",
    "        if i < train_size:\n",
    "            x_train[i, t, :] = x_vectors[token]\n",
    "        else:\n",
    "            x_test[i - train_size, t, :] = x_vectors[token]\n",
    "\n",
    "  \n",
    "    if i < train_size:\n",
    "        y_train[i, :] = [1.0, 0.0] if labels[index] == 0 else [0.0, 1.0]\n",
    "    else:\n",
    "        y_test[i - train_size, :] = [1.0, 0.0] if labels[index] == 0 else [0.0, 1.0]\n",
    "    \n",
    "del tweets\n",
    "del labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1016,
     "status": "ok",
     "timestamp": 1532866461108,
     "user": {
      "displayName": "MANDAV",
      "photoUrl": "//lh4.googleusercontent.com/-Zi5_ERBmNoM/AAAAAAAAAAI/AAAAAAAAASc/1pLc_S0nstI/s50-c-k-no/photo.jpg",
      "userId": "113535474679329766874"
     },
     "user_tz": -330
    },
    "id": "ntJKWYdRxURp",
    "outputId": "11f65810-0b07-449c-bfb5-7019961fbe7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((360000, 15, 256), (40000, 2))"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4KtiWWSSEP8Z"
   },
   "source": [
    "### Building the Neural Model\n",
    "\n",
    "For training a combination of Convolution Neural Network and Bidirectional Long Short Term Memory Network is used (CNN-LSTM).\n",
    "\n",
    "Batch Size is 100.\n",
    "\n",
    "\n",
    "To prevent overfitting or over training of the network, `EarlyStopping()` is used in `callbacks` thus if the network does not improve or starts overfitting, the training comes to an end.\n",
    "\n",
    "**Acrhitecture of Network:**\n",
    "\n",
    "===============================================================================\n",
    "\n",
    "Conv1D -> Conv1D -> Conv1D -> Max Pooling1D -> Bidirectional LSTM -> Dense -> Dropout -> Dense -> Dropout -> Dense -> Dropout -> Output\n",
    "\n",
    "===============================================================================\n",
    "\n",
    "Total params: 3,314,274\n",
    "\n",
    "Trainable params: 3,314,274\n",
    "\n",
    "Non-trainable params: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hCLQdrPoC9X8"
   },
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "no_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13602,
     "status": "ok",
     "timestamp": 1532866479701,
     "user": {
      "displayName": "MANDAV",
      "photoUrl": "//lh4.googleusercontent.com/-Zi5_ERBmNoM/AAAAAAAAAAI/AAAAAAAAASc/1pLc_S0nstI/s50-c-k-no/photo.jpg",
      "userId": "113535474679329766874"
     },
     "user_tz": -330
    },
    "id": "_wiKyELIxrsa",
    "outputId": "0c96976b-d893-4f94-bd58-98cb4dcdb50e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 15, 32)            24608     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 15, 32)            3104      \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 15, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5, 32)             0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1024)              2232320   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 3,314,274\n",
      "Trainable params: 3,314,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, Dropout, Dense, Flatten, LSTM, MaxPooling1D, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same',\n",
    "                 input_shape=(max_no_tokens, vector_size)))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "\n",
    "model.add(Bidirectional(LSTM(512, dropout=0.2, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001, decay=1e-6), metrics=['accuracy'])\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='logs/', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z-QiTfk4Em-z"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1312
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2195081,
     "status": "ok",
     "timestamp": 1532868689127,
     "user": {
      "displayName": "MANDAV",
      "photoUrl": "//lh4.googleusercontent.com/-Zi5_ERBmNoM/AAAAAAAAAAI/AAAAAAAAASc/1pLc_S0nstI/s50-c-k-no/photo.jpg",
      "userId": "113535474679329766874"
     },
     "user_tz": -330
    },
    "id": "IPfQ-P0y3znW",
    "outputId": "358807d3-34cf-4963-caee-82d317b96acb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Train on 360000 samples, validate on 40000 samples\n",
      "Epoch 1/100\n",
      "153000/360000 [===========>..................] - ETA: 38s - loss: 0.7116 - acc: 0.5368360000/360000 [==============================] - 64s 177us/step - loss: 0.6803 - acc: 0.5819 - val_loss: 0.6263 - val_acc: 0.6465\n",
      "Epoch 2/100\n",
      "141000/360000 [==========>...................] - ETA: 35s - loss: 0.6362 - acc: 0.6343360000/360000 [==============================] - 60s 166us/step - loss: 0.6313 - acc: 0.6394 - val_loss: 0.6113 - val_acc: 0.6598\n",
      "Epoch 3/100\n",
      "136500/360000 [==========>...................] - ETA: 36s - loss: 0.6198 - acc: 0.6506360000/360000 [==============================] - 60s 168us/step - loss: 0.6177 - acc: 0.6516 - val_loss: 0.6129 - val_acc: 0.6554\n",
      "Epoch 4/100\n",
      "134500/360000 [==========>...................] - ETA: 36s - loss: 0.6121 - acc: 0.6560360000/360000 [==============================] - 61s 169us/step - loss: 0.6099 - acc: 0.6584 - val_loss: 0.5996 - val_acc: 0.6690\n",
      "Epoch 5/100\n",
      "134000/360000 [==========>...................] - ETA: 36s - loss: 0.6056 - acc: 0.6633360000/360000 [==============================] - 60s 167us/step - loss: 0.6043 - acc: 0.6639 - val_loss: 0.5963 - val_acc: 0.6700\n",
      "Epoch 6/100\n",
      "134000/360000 [==========>...................] - ETA: 36s - loss: 0.6011 - acc: 0.6683360000/360000 [==============================] - 60s 167us/step - loss: 0.5990 - acc: 0.6697 - val_loss: 0.5917 - val_acc: 0.6741\n",
      "Epoch 7/100\n",
      "134000/360000 [==========>...................] - ETA: 36s - loss: 0.5948 - acc: 0.6721360000/360000 [==============================] - 61s 168us/step - loss: 0.5955 - acc: 0.6716 - val_loss: 0.5882 - val_acc: 0.6774\n",
      "Epoch 8/100\n",
      "134000/360000 [==========>...................] - ETA: 36s - loss: 0.5943 - acc: 0.6721360000/360000 [==============================] - 60s 167us/step - loss: 0.5927 - acc: 0.6742 - val_loss: 0.5870 - val_acc: 0.6778\n",
      "Epoch 9/100\n",
      "134000/360000 [==========>...................] - ETA: 36s - loss: 0.5894 - acc: 0.6790360000/360000 [==============================] - 61s 168us/step - loss: 0.5892 - acc: 0.6772 - val_loss: 0.5854 - val_acc: 0.6787\n",
      "Epoch 10/100\n",
      "133000/360000 [==========>...................] - ETA: 36s - loss: 0.5884 - acc: 0.6784360000/360000 [==============================] - 60s 166us/step - loss: 0.5877 - acc: 0.6779 - val_loss: 0.5848 - val_acc: 0.6786\n",
      "Epoch 11/100\n",
      "133000/360000 [==========>...................] - ETA: 36s - loss: 0.5852 - acc: 0.6819360000/360000 [==============================] - 60s 166us/step - loss: 0.5853 - acc: 0.6806 - val_loss: 0.5810 - val_acc: 0.6807\n",
      "Epoch 12/100\n",
      "132000/360000 [==========>...................] - ETA: 38s - loss: 0.5842 - acc: 0.6828360000/360000 [==============================] - 62s 172us/step - loss: 0.5837 - acc: 0.6818 - val_loss: 0.5807 - val_acc: 0.6822\n",
      "Epoch 13/100\n",
      "130500/360000 [=========>....................] - ETA: 38s - loss: 0.5813 - acc: 0.6848360000/360000 [==============================] - 61s 171us/step - loss: 0.5822 - acc: 0.6832 - val_loss: 0.5785 - val_acc: 0.6828\n",
      "Epoch 14/100\n",
      "132000/360000 [==========>...................] - ETA: 37s - loss: 0.5791 - acc: 0.6857360000/360000 [==============================] - 62s 171us/step - loss: 0.5800 - acc: 0.6846 - val_loss: 0.5786 - val_acc: 0.6846\n",
      "Epoch 15/100\n",
      "132500/360000 [==========>...................] - ETA: 37s - loss: 0.5795 - acc: 0.6841360000/360000 [==============================] - 62s 171us/step - loss: 0.5792 - acc: 0.6854 - val_loss: 0.5774 - val_acc: 0.6854\n",
      "Epoch 16/100\n",
      "132000/360000 [==========>...................] - ETA: 37s - loss: 0.5790 - acc: 0.6847360000/360000 [==============================] - 61s 170us/step - loss: 0.5778 - acc: 0.6863 - val_loss: 0.5819 - val_acc: 0.6817\n",
      "Epoch 17/100\n",
      "132500/360000 [==========>...................] - ETA: 37s - loss: 0.5768 - acc: 0.6876360000/360000 [==============================] - 61s 170us/step - loss: 0.5766 - acc: 0.6883 - val_loss: 0.5749 - val_acc: 0.6878\n",
      "Epoch 18/100\n",
      "131500/360000 [=========>....................] - ETA: 37s - loss: 0.5758 - acc: 0.6867360000/360000 [==============================] - 64s 177us/step - loss: 0.5757 - acc: 0.6881 - val_loss: 0.5763 - val_acc: 0.6863\n",
      "Epoch 19/100\n",
      "132000/360000 [==========>...................] - ETA: 37s - loss: 0.5737 - acc: 0.6914360000/360000 [==============================] - 63s 174us/step - loss: 0.5742 - acc: 0.6896 - val_loss: 0.5743 - val_acc: 0.6887\n",
      "Epoch 20/100\n",
      "132000/360000 [==========>...................] - ETA: 38s - loss: 0.5734 - acc: 0.6917360000/360000 [==============================] - 62s 173us/step - loss: 0.5736 - acc: 0.6907 - val_loss: 0.5728 - val_acc: 0.6893\n",
      "Epoch 21/100\n",
      "132500/360000 [==========>...................] - ETA: 37s - loss: 0.5717 - acc: 0.6916360000/360000 [==============================] - 61s 168us/step - loss: 0.5722 - acc: 0.6909 - val_loss: 0.5733 - val_acc: 0.6889\n",
      "Epoch 22/100\n",
      "132500/360000 [==========>...................] - ETA: 36s - loss: 0.5686 - acc: 0.6945360000/360000 [==============================] - 60s 167us/step - loss: 0.5711 - acc: 0.6922 - val_loss: 0.5752 - val_acc: 0.6887\n",
      "Epoch 23/100\n",
      "132500/360000 [==========>...................] - ETA: 36s - loss: 0.5716 - acc: 0.6930360000/360000 [==============================] - 60s 167us/step - loss: 0.5705 - acc: 0.6930 - val_loss: 0.5720 - val_acc: 0.6895\n",
      "Epoch 24/100\n",
      "132500/360000 [==========>...................] - ETA: 36s - loss: 0.5699 - acc: 0.6923360000/360000 [==============================] - 60s 167us/step - loss: 0.5695 - acc: 0.6933 - val_loss: 0.5734 - val_acc: 0.6879\n",
      "Epoch 25/100\n",
      "132500/360000 [==========>...................] - ETA: 37s - loss: 0.5682 - acc: 0.6951360000/360000 [==============================] - 61s 168us/step - loss: 0.5686 - acc: 0.6934 - val_loss: 0.5700 - val_acc: 0.6911\n",
      "Epoch 26/100\n",
      "132500/360000 [==========>...................] - ETA: 36s - loss: 0.5670 - acc: 0.6941360000/360000 [==============================] - 60s 168us/step - loss: 0.5680 - acc: 0.6945 - val_loss: 0.5698 - val_acc: 0.6911\n",
      "Epoch 27/100\n",
      "132000/360000 [==========>...................] - ETA: 36s - loss: 0.5663 - acc: 0.6965360000/360000 [==============================] - 60s 167us/step - loss: 0.5674 - acc: 0.6957 - val_loss: 0.5722 - val_acc: 0.6881\n",
      "Epoch 28/100\n",
      "132500/360000 [==========>...................] - ETA: 36s - loss: 0.5660 - acc: 0.6958360000/360000 [==============================] - 60s 166us/step - loss: 0.5659 - acc: 0.6962 - val_loss: 0.5690 - val_acc: 0.6923\n",
      "Epoch 29/100\n",
      "132500/360000 [==========>...................] - ETA: 37s - loss: 0.5649 - acc: 0.6956360000/360000 [==============================] - 60s 168us/step - loss: 0.5658 - acc: 0.6960 - val_loss: 0.5679 - val_acc: 0.6936\n",
      "Epoch 30/100\n",
      "132500/360000 [==========>...................] - ETA: 37s - loss: 0.5647 - acc: 0.6975360000/360000 [==============================] - 60s 167us/step - loss: 0.5655 - acc: 0.6969 - val_loss: 0.5692 - val_acc: 0.6929\n",
      "Epoch 31/100\n",
      "132500/360000 [==========>...................] - ETA: 36s - loss: 0.5648 - acc: 0.6957360000/360000 [==============================] - 60s 167us/step - loss: 0.5650 - acc: 0.6967 - val_loss: 0.5670 - val_acc: 0.6926\n",
      "Epoch 32/100\n",
      "132000/360000 [==========>...................] - ETA: 37s - loss: 0.5639 - acc: 0.6972360000/360000 [==============================] - 61s 168us/step - loss: 0.5640 - acc: 0.6974 - val_loss: 0.5679 - val_acc: 0.6928\n",
      "Epoch 33/100\n",
      "132500/360000 [==========>...................] - ETA: 36s - loss: 0.5628 - acc: 0.6981360000/360000 [==============================] - 60s 166us/step - loss: 0.5634 - acc: 0.6984 - val_loss: 0.5667 - val_acc: 0.6941\n",
      "Epoch 34/100\n",
      "132500/360000 [==========>...................] - ETA: 36s - loss: 0.5621 - acc: 0.6995360000/360000 [==============================] - 60s 166us/step - loss: 0.5629 - acc: 0.6987 - val_loss: 0.5681 - val_acc: 0.6912\n",
      "Epoch 35/100\n",
      "132500/360000 [==========>...................] - ETA: 36s - loss: 0.5616 - acc: 0.6998360000/360000 [==============================] - 60s 167us/step - loss: 0.5623 - acc: 0.6998 - val_loss: 0.5667 - val_acc: 0.6923\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132500/360000 [==========>...................] - ETA: 36s - loss: 0.5618 - acc: 0.6993360000/360000 [==============================] - 60s 167us/step - loss: 0.5616 - acc: 0.6996 - val_loss: 0.5668 - val_acc: 0.6947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe0a4ca9860>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=batch_size, shuffle=True, epochs=no_epochs,\n",
    "         validation_data=(x_test, y_test), callbacks=[tensorboard, EarlyStopping(min_delta=0.0001, patience=3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TowrJ3QGEqax"
   },
   "source": [
    "### Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1064,
     "status": "ok",
     "timestamp": 1532868690251,
     "user": {
      "displayName": "MANDAV",
      "photoUrl": "//lh4.googleusercontent.com/-Zi5_ERBmNoM/AAAAAAAAAAI/AAAAAAAAASc/1pLc_S0nstI/s50-c-k-no/photo.jpg",
      "userId": "113535474679329766874"
     },
     "user_tz": -330
    },
    "id": "tQmcfrwu5pak",
    "outputId": "1bb47b2f-1657-4e22-984c-c63adf364103"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20466,
     "status": "ok",
     "timestamp": 1532868710794,
     "user": {
      "displayName": "MANDAV",
      "photoUrl": "//lh4.googleusercontent.com/-Zi5_ERBmNoM/AAAAAAAAAAI/AAAAAAAAASc/1pLc_S0nstI/s50-c-k-no/photo.jpg",
      "userId": "113535474679329766874"
     },
     "user_tz": -330
    },
    "id": "9hAXHL915SPX",
    "outputId": "98c41a16-c314-4eb9-bff9-1eb359de5052"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 19s 487us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5668408149003983, 0.694675]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=x_test, y=y_test, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vPQ4ofRjE0Gk"
   },
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "FDgLrrbu7uAX"
   },
   "outputs": [],
   "source": [
    "model.save('twitter-sentiment-word2vec-400k.model')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Untitled0.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
