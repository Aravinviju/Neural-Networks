# -*- coding: utf-8 -*-
"""cnn-handwritten-mnist-dataset.ipynb

Automatically generated by Colaboratory.


By: Jatin Kumar Mandav

Module: TensorFlow

Website: https://jatinmandav.wordpress.com
YouTube Channel: https://youtube.com/mandav
Twitter: @jatinmandav



# **Building Convolutional Neural Network Using [Tensorflow](https://www.tensorflow.org/)**


Let's Build a simple CNN for Handwritten numbers from MNIST Dataset (<http://yann.lecun.com/exdb/mnist/>)

CNN Architecture that I wil be Building is following:
  - Convolution Layer 1: 5x5 Kernel
  - Max - pool Layer: 2x2 Kernel
  - Convolution Layer 2: 5x5 Kernel
  - Convolution Layer 3: 5x5 Kernel
  - Convolution Layer 4: 5x5 Kernel
  - Max - pool Layer: 2x2 Kernel
  - Convolution Layer 5: 5x5 Kernel
  - Convolution Layer 6: 5x5 Kernel
  - Fully Connected Layer with 1024 Neurons
  - Output Layer with 10 Neurons
  
  
## **CNN Layout**

> ***Conv1*** --> ***ReLU*** --> ***MaxPool*** --> ***Conv1*** --> ***ReLU*** --> ***Conv1*** --> ***ReLU*** --> ***Conv1*** --> ***ReLU*** --> ***MaxPool*** --> ***Conv1*** --> ***ReLU*** --> ***Conv1*** --> ***ReLU*** --> ***FullyConnected*** --> ***ReLU*** ---> ***OutputLayer***



**Optimizer:** Adam Optimizer

**Activation Function:** ReLU

**Cost Function:** Softmax - Cross Entropy
"""

# Imports!
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets("/tmp/data", one_hot=True)

no_prediction_classes = 10
batch_size = 128

# Place Holders for input image (x) and prediction or correct identification (y)
x = tf.placeholder('float', [None, 784])
y = tf.placeholder('float')

# Drop - Out Place Holder
keep_prob = tf.placeholder(tf.float32)

"""**Defining functions for processing using TensorFlow Library!**

  -- tf.nn.**conv2d**(input, filter, strides, padding, use_sudnn_on_gpu=True, data_format='NHWC', dilations=[1, 1, 1, 1], name=None)
  
  -- tf.nn.**max_pool**(value, ksize, strides, padding, data_format='NHWC', name=None)
  
  -- tf.nn.**relu**(features, name=None)
  
  -- tf.nn.**dropout**(x, keep_prob, noise_shape=None, seed=None, name=None)
"""

# 2D Convolution Function
def conv2d(x, filters):
  return tf.nn.conv2d(x, filters, strides=[1, 1, 1, 1], padding='SAME')

# 2D Max - Pool Function
def max_pool2d(x):
  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

# Activation Function
def relu(x):
    return tf.nn.relu(x)

# Drop out Function
def dropout(x, keep_rate):
  return tf.nn.dropout(x, keep_rate)

"""**MNIST** Dataset is of the form of **1x785** array (or list), which needs to be converted into **28x28x1** 2D Image to pass through a CNN.

### **Calculating the Padding**


If Padding is ***SAME***:

    out_height = ceil(float(in_height)/float(stride[1]))
    out_width = ceil(float(in_width)/float(stride[2]))
    if in_height % stride[1] == 0:
        pad_along_height = max(filter_height - stride[1], 0)
    else:
        pad_along_height = max(filter_height - (in_height % stride[1]), 0)
    if in_width % stride[2] == 0:
        pad_along_width = max(filter_width - stride[2], 0)
    else:
        pad_along_width = max(filter_width - (in_width % stride[2]), 0)
        padding_top = pad_along_height // 2
        padding_bottom = pad__along_height - padding_top
        padding_left = pad_along_width // 2
        padding_right = pad__along_width - padding_left    

If Padding is ***VALID***:

    out_height = ceil(float(in_height - filter_height + 1)/float(stride[1]))
    out_width = ceil(float(in_width - filter_width + 1)/float(stride[2]))
    No Padding.
    

  -- Input Tensor Format: [Batch, Height, Width, Channels]

  -- Kernel or Filter Format: [Height, Width, In_Channels, Out_Channels]
"""

# Building the Convolutional Neural Network
def convolutional_neural_network(x):
  # Reshaping MNIST Dataset
  x = tf.reshape(x, shape=[-1, 28, 28, 1])
  
  # Input Size for Layer 1 = [1, 28, 28, 1]
  weights_conv1 = tf.Variable(tf.random_normal([5, 5, 1, 32]))
  biases_conv1 = tf.Variable(tf.random_normal([32]))
  
  conv1 = relu(conv2d(x, weights_conv1) + biases_conv1)
  
  # Max - Pool 1
  conv1 = max_pool2d(conv1)
  
  # Input Size for Layer 2 = [1, 14, 14, 32]
  weights_conv2 = tf.Variable(tf.random_normal([5, 5, 32, 32]))
  biases_conv2 = tf.Variable(tf.random_normal([32]))
  
  conv2 = relu(conv2d(conv1, weights_conv2) + biases_conv2)
  
  # Input Size for Layer 3 = [1, 14, 14, 32]
  weights_conv3 = tf.Variable(tf.random_normal([5, 5, 32, 32]))
  biases_conv3 = tf.Variable(tf.random_normal([32]))
  
  conv3 = relu(conv2d(conv2, weights_conv3) + biases_conv3)
  
  # Input Size for Layer 4 = [1, 14, 14, 32]
  weights_conv4 = tf.Variable(tf.random_normal([5, 5, 32, 64]))
  biases_conv4 = tf.Variable(tf.random_normal([64]))
  
  conv4 = relu(conv2d(conv3, weights_conv4) + biases_conv4)
  
  # Max - Pool 2
  conv4 = max_pool2d(conv4)
  
  # Input Size for Layer 5 = [1, 7, 7, 64]
  weights_conv5 = tf.Variable(tf.random_normal([5, 5, 64, 64]))
  biases_conv5 = tf.Variable(tf.random_normal([64]))
  
  conv5 = relu(conv2d(conv4, weights_conv5) + biases_conv5)
  
  # Input Size for Layer 6 = [1, 7, 7, 64]
  weights_conv6 = tf.Variable(tf.random_normal([5, 5, 64, 64]))
  biases_conv6 = tf.Variable(tf.random_normal([64]))
  
  conv6 = relu(conv2d(conv5, weights_conv6) + biases_conv6)
  
  # Input Size for Fully Connected Layer = 7*7*64
  weights_fc = tf.Variable(tf.random_normal([7*7*64, 1024]))
  biases_fc = tf.Variable(tf.random_normal([1024]))
  
  # Flattening Conv6 Layer
  conv6 = tf.reshape(conv6, shape=[-1, 7*7*64])
  
  fc = relu(tf.matmul(conv6, weights_fc) + biases_fc)
  
  # Drop - Out Some Neurons!
  keep_rate = 0.8
  
  fc = dropout(fc, keep_rate)
  
  # Input Size for Output Layer = [1024]
  weights_out = tf.Variable(tf.random_normal([1024, no_prediction_classes]))
  biases_out = tf.Variable(tf.random_normal([no_prediction_classes]))
  
  out_layer = tf.matmul(fc, weights_out) + biases_out
  
  # Returning the Output Prediction by out little CNN!
  return out_layer

"""## Training!

  - **Cost Function:** tf.nn.softmax_cross_entropy_with_logits_v2(_sentinel=None, labels=None, logits=None, dim=-1, name=None)

  - **Optimizer:** tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')
"""

# Training Function!
def train_cnn(x):
  # Prediction by CNN
  prediction = convolutional_neural_network(x)
  
  # Cost Function
  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=y))
  
  # Optimizer
  optimizer = tf.train.AdamOptimizer().minimize(cost)
  
  total_epochs = 20
  
  with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    
    for epoch in range(total_epochs):
      epoch_loss = 0
      for _ in range(int(mnist.train.num_examples/batch_size)):
        epoch_x, epoch_y = mnist.train.next_batch(batch_size)
        
        _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})
        
        epoch_loss += c
        
      print('Epoch: ', epoch, ', Loss: ', epoch_loss)
      
    correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))
    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))
    
    for _ in range(10):
      test_x, test_y = mnist.test.next_batch(50)
      print('Test Accuracy: ', accuracy.eval({x: test_x, y: test_y}))

"""## **Let's Train Our Little Convolutional Neural Network!**"""

if __name__ == "__main__":
  train_cnn(x)